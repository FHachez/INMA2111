\part*{TEMPORARY - CM1}

\setcounter{section}{-1}

\section{Reminders : Asymptotic notation}

Let 
$ \left\{
\begin{array}{ll}
f :& \mathbb{N} \rightarrow \mathbb{R}^+\\
g :& \mathbb{N} \rightarrow \mathbb{R}^+
\end{array} \right.$ 

\begin{itemize}
\renewcommand{\labelitemi}{$\bullet$}
	\item  $f \in \mathcal{O}(g)$, often written $f(n) = \mathcal{O}(g(n))$, means: $\exists n_0, c : \forall n \geq _0, f(n) \leq c \cdot g(n)$.
	\item 	
	$f \in \Theta(g)$ $\begin{array}{l} 
	   \Leftrightarrow f \in \mathcal{O}(g)  \text{ and } g \in \mathcal{O}(f) \\
	\Leftrightarrow  \exists n_0, c_1, c_2 : \forall n\geq n_0, c_1 \cdot g(n)\leq f(n) \leq c_2 \cdot g(n) 
	\end{array}$	
	($f$ and $g$ are equivalent in complexity)
\item $f \in o(g) \Leftrightarrow f \in \mathcal(O)(g)$ but $f \notin \Theta(g)$
\item  $f \in \Omega(g) \Leftrightarrow g \in \mathcal{O}(f)$
\item $f \in \omega(g) \Leftrightarrow g \in o(f)$
\end{itemize}

\begin{example}
\begin{align*} 
	   n^2 & = \mathcal{O}(n^2) &\\
		 & = o(n^3) &\\
		 & = \Theta \left(\frac{n (n-1)}{2}\right) &\\
		& = \Omega (n \log (n))& 
	\end{align*}
\end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Sorting algorithms}

We will start with illustrating some ideas on sorting algorithms. The sorting problem may be defined by its input and output. Hence our goal is to find some algorithms such that: \newline

\textbf{Input :} an array $T = \begin{pmatrix}T(1) & \ldots & T(n) \end{pmatrix}$. 

\textbf{Output :} the array is sorted in increasing order. 

\subsection{Selection Sort}

First, we consider a naÃ¯ve algorithm: \textbf{Selection Sort}. This first algorithm can be formulated as follows:

\begin{itemize}
\renewcommand{\labelitemi}{$\bullet$}
	\item Find the smallest entry of $T$.
	\item Put it in the first position.	
	\item Repeat this procedure iteratively: Find the smallest entry of the rest, put it in the second position, etc
\end{itemize}

What about the time complexity of this algorithm?
\begin{align*}
\text{Time } &= \Theta(n) + \Theta(n-1) + \ldots + \Theta(1) \\
& = \Theta(n^2)
\end{align*}

\begin{remark}
The complexity of this algorithm is $\Theta(n^2)$ for each instance. We have Worst case $=$ Best case $=$ Average case time complexity $= \Theta(n^2)$.
\end{remark}

\subsection{Insertion Sort}
\begin{itemize}
\renewcommand{\labelitemi}{$\bullet$}
	\item \textbf{For} $i = 2$ to $n$ \hspace{1cm} \% $T(1), \ldots , T(i-1)$ are already sorted = "Loop-invariant".
  \item Shift $T(i)$ to the left by recursive swaps until well paced.
\end{itemize}

\begin{example}
We can consider a small exemple, the first four elements of the array being sorted: 
\tikzstyle{block} = [rectangle, draw, 
    text width=0.8cm, text centered]
\begin{tikzpicture}[node distance = 1cm, auto]
    % Place nodes
    \node [block] (one) {1};
    \node [block, right of=one] (two) {3};
    \node [block, right of=two] (three) {5};
    \node [block, right of=three] (four) {9};
    \node [block, right of=four] (five) {\textcolor[rgb]{0,0,1}{4}};
    \node [block, right of=five] (six) {8};
    \node [block, right of=six] (seven) {0};
		\node[rectangle, below of=five] (ind) {i=5};
    % Draw edges
		\draw[->,distance =8pt, thick,draw=blue] (five.north)to[bend right](four.north);
		\draw[->,distance =8pt,thick, draw=blue] (four.north)to[bend right](three.north);
		\draw[->,thick] (ind.north)to(five.south);
\end{tikzpicture}
\end{example}

Regarding the time complexity, we have: 
\begin{itemize}
\renewcommand{\labelitemi}{$\bullet$}
	\item in worst case, at every step we swap until the beginning of the array: worst-case complexity $= \Theta(1)+\ldots + \Theta(n) = \Theta(n^2)$. This case corresponds to an instance sorted in decreasing order. 
	\item Best-case corresponds to an array already sorted: $\Theta(n)$
	\item Average-case: instances come with uniform probability distribution over all possible orders ($=n!$ orders). Then:
	\begin{flalign*}
		\mathbb{E}[Time] & = \mathbb{E}[\sum_{i=2}^n \Theta(t_i)] \hspace{1cm} \text{  With $t_i$ the number of swaps needed for $T(i)$.}& \\
		& = \Theta \left( \sum_{i=2}^n \mathbb{E}[t_i]\right)  \hspace{1cm} \text{ By linearity of $\mathbb{E}$.}&\\
		& = \Theta \left( \sum_{i=2}^n \frac{i}{2}\right) & \\
		& = \Theta(n^2)
	\end{flalign*}
\end{itemize}

In \emph{some} applications, the relevant probability distribution is not uniform but biased towards almost-sorted instances.   \newline 
$\Rightarrow$ Average-case can be $o(n^2)$. 

\subsection{Quick Sort}
This third algorithm is based on the idea of "Divide-and-Conquer" which solves the sorting problem in the following manner:

\begin{itemize}
\renewcommand{\labelitemi}{$\bullet$}
	\item Split into small sorting problems.
	\item Recombine the outputs, which gives the sortd instance.
\end{itemize}

In particular, in this case:

\begin{itemize}
\renewcommand{\labelitemi}{$\bullet$}
	\item $Pivot:=T(1)$.
	\item $T_{low}:= [T(i): T(i)\leq Pivot]$.
	\item $T_{high}:= [T(i): T(i)>Pivot]$.
	\item $\left.
	\begin{array}{ll}
       \rightarrow & \text{ Quick Sort $T_{low}$}.\\
       \rightarrow &\text{ Quick Sort $T_{high}$}.
			\end{array}
  \right\}$ $\Rightarrow T := [T_{low}; T_{high}]$.
  
	(The pivot being in $T_{low}$).
\end{itemize}

About the complexity of this algorithm:
\begin{itemize}
\renewcommand{\labelitemi}{$\bullet$}
\item Worst case: when instance is already sorted ! \newline
In this case: $\left.
	\begin{array}{ll}
       T_{low} & \text{= 1 entry}.\\
       T_{high} &\text{= $n-1$ entries}.
			\end{array}
  \right]$ $\Rightarrow$ maximally unbalanced subproblems. Time $= \Theta(n^2)$ 
	
	Indeed: construct $T_{low}/ T_{right} = \Theta(n)$, hence we have the worst-case recurrence equation: 
	\begin{align*}
t_n & = t_1 + t_{n-1}+ \Theta(n)\\
 & = t_1 + \left( t_1 + t_{n-2} + \Theta(n-1)\right)+ \Theta(n)\\
& = t_1 + \left( t_1 + (t_1 + t_{n-3}+ \Theta(n-2)) + \Theta(n-1)\right)+ \Theta(n)\\
& = \ldots \\
& = n \cdot t_1 + \Theta(n^2) = \Theta(n^2)
\end{align*}

\item Average-case complexity: 
\begin{align}
t_n & = t_{\frac{n}{2}}+ t_{\frac{n}{2}}+ \Theta(n) \label{eq:quickSort_average} \\
& = 2 \cdot t_{\frac{n}{2}} + \Theta(n)  \notag \\
& = 2 \left( 2 \cdot t_{\frac{n}{4}} + \Theta(\frac{n}{2})\right) + \Theta(n)  \notag \\
& =  2 \left( 2 \cdot \left(2 \cdot t_{\frac{n}{8}} + \Theta(\frac{n}{4})\right) + \Theta(\frac{n}{2})\right) + \Theta(n)  \notag \\
& = \ldots  \notag \\
& = 2^{\log_2(n)} \cdot t_1 + \Theta(n \log_2(n))  \notag \\
& = \Theta(n \log_2(n)) \notag
\end{align}
\end{itemize}

\begin{remark}
We will denote $\log_2$ by $\log$, since $\log_a(n)\in \Theta(\log_b(n))$, $\forall a,b >1$. 
\end{remark}

\begin{remark}
In $t_n = t_{\frac{n}{2}}+ t_{\frac{n}{2}} + \Theta(n)$,
\begin{align*}
\mathbb{E}[t_n] & = \mathbb{E}[t_{|T_{low}|}]+ \mathbb{E}[t_{|T_{high}|}] + \Theta(n) \text{ would be more correct.} & \\
& \cong t_{\mathbb{E}[|T_{low}|]} + t_{\mathbb{E}[|T_{high}|]}  + \Theta(n) \text{ : in fact OK.}
\end{align*}
\end{remark}

Now, what if you don't know that your instances are uniformly distributed? This leads to the next algorithm.

\subsection{Randomized Quick Sort}
Here, the first step of the algorithm is to shuffle entries randomly. It is equivalent to picking a random entry as pivot, instead of $T(1)$.

Now on any instance, Randomized Quick Sort runs in expected time $\Theta(n \log(n))$ $\Rightarrow$ Worst-case expected time of this random algorithm is also $\Theta(n \log(n))$. 

One advantage of randomization: Robin Hood effect, i.e. you take from rich instances and give to the poor. Now, everybody can be unlucky from time to time but lucky most of the time. 

Can we reach worst-case complexity $\mathcal{O}(n \log(n))$ deterministically? 
\newline The answer is yes, as we will see in the next section (Merge Sort). 

\subsection{Merge Sort}

This algorithm is also based on Divide-and-Conquer approach. 

\begin{itemize}
\renewcommand{\labelitemi}{$\bullet$}
	\item $T_{left} := [T(1), \ldots, T(\lfloor \frac{n}{2} \rfloor)]$.
	\item $T_{right}:= [T(\lfloor \frac{n}{2}\rfloor +1), \ldots, T(n)]$.
	\item $\left.
	\begin{array}{ll}
       \rightarrow & \text{ Merge Sort $T_{left}$}.\\
       \rightarrow &\text{ Merge Sort $T_{right}$}.
			\end{array}
  \right\}$ $\Rightarrow T := Merge\left(T_{left}, T_{right}\right)$.  
\end{itemize}

Time of merge is $\Theta(n)$, so we have the recurrence equation: 
\begin{align*}
t_n & = 2 \cdot t_{\lceil \frac{n}{2}\rceil}+ \Theta(n) \\
& = \Theta(n \log(n))  \text{ (see equation~\eqref{eq:quickSort_average})}
\end{align*}

Which one to use?
\begin{itemize}
\renewcommand{\labelitemi}{$\bullet$}
	\item Random Quick Sort:
	 \begin{itemize}
		\item \textcircled{-} You need to generate randomness: not so easy for a computer!
		\item \textcircled{-} You can be unlucky.	
		\item \textcircled{+} Hidden constants are smaller if implemented efficiently. 
		\item \textcircled{+} can be implemented "in place": on $T$ itself $\rightarrow$ no need for extra memory for intermediate variables.
	\end{itemize} 
	
\end{itemize}


